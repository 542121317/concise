{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Variant effect prediction\n",
    "The variant effect prediction parts integrated in `concise` are designed to extract importance scores for a single nucleotide variant in a given sequence. Predictions are made for each output individually for a multi-task model. In this short tutorial we will be using a small model to explain the basic functionality and outputs.\n",
    "\n",
    "At the moment there are three different effect scores to be chosen from. All of them require as in input:\n",
    "\n",
    "* The input sequence with the variant with its reference genotype\n",
    "* The input sequence with the variant with its alternative genotype\n",
    "* Both aformentioned sequences in reverse-complement\n",
    "* Information on where (which basepair, 0-based) the mutation is placed in the forward sequences\n",
    "\n",
    "The following variant scores are available:\n",
    "\n",
    "* In-silico mutagenesis (ISM):\n",
    "\t- Predict the outputs of the sequences containing the reference and alternative genotype of the variant and use the differential output as a effect score.\n",
    "\n",
    "* Gradient-based score\n",
    "* Dropout-based score\n",
    "\n",
    "## Calculating effect scores\n",
    "Firstly we will need to have a trained model and a set of input sequences containing the variants we want to look at. For this tutorial we will be using a small model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 1 pwm's from pwm_list\n",
      "Removing 1 pwm's from pwm_list\n",
      "Train on 3713 samples, validate on 3713 samples\n",
      "Epoch 1/1\n",
      "3713/3713 [==============================] - 2s - loss: 0.4300 - mean_squared_error: 0.4299 - val_loss: 0.2329 - val_mean_squared_error: 0.2328\n"
     ]
    }
   ],
   "source": [
    "from effect_demo_setup import *\n",
    "from concise.models import single_layer_pos_effect as concise_model\n",
    "import numpy as np\n",
    "\n",
    "# Generate training data for the model, use a 1000bp sequence\n",
    "param, X_feat, X_seq, y, id_vec = load_example_data(trim_seq_len = 1000)\n",
    "\n",
    "# Generate the model\n",
    "dc = concise_model(pooling_layer=\"sum\",\n",
    "                   init_motifs=[\"TGCGAT\", \"TATTTAT\"],\n",
    "                   n_splines=10,\n",
    "                   n_covariates=0,\n",
    "                   seq_length=X_seq.shape[1],\n",
    "                   **param)\n",
    "\n",
    "# Train the model\n",
    "dc.fit([X_seq], y, epochs=1,\n",
    "       validation_data=([X_seq], y))\n",
    "    \n",
    "# In order to select the right output of a potential multitask model we have to generate a list of output labels, which will be used alongside the model itself.\n",
    "model_output_annotation = np.array([\"output_1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with any prediction that you want to make with a model it is necessary that the input sequences have to fit the input dimensions of your model, in this case the reference and alternative sequences in their forward and reverse-complement state have to have the shape [?, 1000, 4].\n",
    "\n",
    "We will be storing the dataset in a dictionary for convenience:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "dataset_path = \"%s/data/sample_hqtl_res.hdf5\"%concise_demo_data_path\n",
    "dataset = {}\n",
    "with h5py.File(dataset_path, \"r\") as ifh:\n",
    "    ref = ifh[\"test_in_ref\"].value\n",
    "    alt = ifh[\"test_in_alt\"].value\n",
    "    dirs = ifh[\"test_out\"][\"seq_direction\"].value\n",
    "    \n",
    "    # This datset is stored with forward and reverse-complement sequences in an interlaced manner\n",
    "    assert(dirs[0] == b\"fwd\")\n",
    "    dataset[\"ref\"] = ref[::2,...]\n",
    "    dataset[\"alt\"] = alt[::2,...]\n",
    "    dataset[\"ref_rc\"] = ref[1::2,...]\n",
    "    dataset[\"alt_rc\"] = alt[1::2,...]\n",
    "    dataset[\"y\"] = ifh[\"test_out\"][\"type\"].value[::2]\n",
    "    \n",
    "    # The sequence is centered around the mutatiom with the mutation occuring on position when looking at forward sequences\n",
    "    dataset[\"mutation_position\"] = np.array([500]*dataset[\"ref\"].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All prediction functions have the same general set of required input values. Before going into more detail of the individual prediction functions We will look into how to run them. The following input arguments are availble for all functions:\n",
    "\n",
    "\tmodel: Keras model\n",
    "\tref: Input sequence with the reference genotype in the mutation position\n",
    "\tref_rc: Reverse complement of the 'ref' argument\n",
    "\talt: Input sequence with the alternative genotype in the mutation position\n",
    "\talt_rc: Reverse complement of the 'alt' argument\n",
    "\tmutation_positions: Position on which the mutation was placed in the forward sequences\n",
    "\tout_annotation_all_outputs: Output labels of the model.\n",
    "\tout_annotation: Select for which of the outputs (in case of a multi-task model) the predictions should be calculated.\n",
    "\n",
    "The `out_annotation` argument is not required. We will now run the available predictions individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 1 pwm's from pwm_list\n",
      "Removing 1 pwm's from pwm_list\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/avsec/bin/anaconda3/lib/python3.5/site-packages/scipy/stats/_distn_infrastructure.py:879: RuntimeWarning: invalid value encountered in greater\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/home/avsec/bin/anaconda3/lib/python3.5/site-packages/scipy/stats/_distn_infrastructure.py:879: RuntimeWarning: invalid value encountered in less\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/home/avsec/bin/anaconda3/lib/python3.5/site-packages/scipy/stats/_distn_infrastructure.py:1818: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= self.a)\n",
      "/home/avsec/bin/anaconda3/lib/python3.5/site-packages/concise/effects/dropout.py:183: RuntimeWarning: invalid value encountered in greater\n",
      "  sel = (np.abs(prob) > np.abs(prob_rc)).astype(np.int)  # Select the LOWER p-value among fwd and rc\n"
     ]
    }
   ],
   "source": [
    "from concise.effects.ism import ism\n",
    "from concise.effects.gradient import gradient_pred\n",
    "from concise.effects.dropout import dropout_pred\n",
    "\n",
    "ism_result = ism(model = dc, \n",
    "                 ref = dataset[\"ref\"], \n",
    "                 ref_rc = dataset[\"ref_rc\"], \n",
    "                 alt = dataset[\"alt\"], \n",
    "                 alt_rc = dataset[\"alt_rc\"], \n",
    "                 mutation_positions = dataset[\"mutation_position\"], \n",
    "                 out_annotation_all_outputs = model_output_annotation, diff_type = \"diff\")\n",
    "gradient_result = gradient_pred(model = dc, \n",
    "                                ref = dataset[\"ref\"], \n",
    "                                ref_rc = dataset[\"ref_rc\"], \n",
    "                                alt = dataset[\"alt\"], \n",
    "                                alt_rc = dataset[\"alt_rc\"], \n",
    "                                mutation_positions = dataset[\"mutation_position\"], \n",
    "                                out_annotation_all_outputs = model_output_annotation)\n",
    "dropout_result = dropout_pred(model = dc, \n",
    "                              ref = dataset[\"ref\"], \n",
    "                              ref_rc = dataset[\"ref_rc\"], \n",
    "                              alt = dataset[\"alt\"], alt_rc = dataset[\"alt_rc\"], mutation_positions = dataset[\"mutation_position\"], out_annotation_all_outputs = model_output_annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alt':      output_1\n",
       " 0      0.0051\n",
       " 1      0.0051\n",
       " 2      0.0000\n",
       " ..        ...\n",
       " 753    0.0048\n",
       " 754    0.0037\n",
       " 755    0.0096\n",
       " \n",
       " [756 rows x 1 columns], 'diff':      output_1\n",
       " 0      0.0051\n",
       " 1      0.0051\n",
       " 2     -0.0050\n",
       " ..        ...\n",
       " 753    0.0048\n",
       " 754   -0.0010\n",
       " 755    0.0060\n",
       " \n",
       " [756 rows x 1 columns], 'ref':      output_1\n",
       " 0      0.0000\n",
       " 1      0.0000\n",
       " 2      0.0050\n",
       " ..        ...\n",
       " 753    0.0000\n",
       " 754    0.0047\n",
       " 755    0.0036\n",
       " \n",
       " [756 rows x 1 columns]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of all functions is a dictionary, please refer to the individual chapters further on for an explanation of the individual values. Every dictionary contains pandas dataframes as values. Every column of the dataframe is named according to the values given in the `out_annotation_all_outputs` labels and contains the respective predicted scores.\n",
    "\n",
    "### Convenience function\n",
    "For convenience there is also a function available which enables the execution of all functions in one call.\n",
    "Additional arguments of the `effect_from_model` function are:\n",
    "\n",
    "\tmethods: A list of prediction functions to be executed. Using the same function more often than once (even with different parameters) will overwrite the results of the previous calculation of that function.\n",
    "\textra_args: None or a list of the same length as 'methods'. The elements of the list are dictionaries with additional arguments that should be passed on to the respective functions in 'methods'. Arguments defined here will overwrite arguments that are passed to all methods.\n",
    "\t**argv: Additional arguments to be passed on to all methods, e.g,: out_annotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from concise.effects.snp_effects import effect_from_model\n",
    "\n",
    "# Define the parameters:\n",
    "params = {\"methods\": [gradient_pred, dropout_pred, ism],\n",
    "         \"model\": dc,\n",
    "         \"ref\": dataset[\"ref\"],\n",
    "         \"ref_rc\": dataset[\"ref_rc\"],\n",
    "         \"alt\": dataset[\"alt\"],\n",
    "         \"alt_rc\": dataset[\"alt_rc\"],\n",
    "         \"mutation_positions\": dataset[\"mutation_position\"],\n",
    "         \"extra_args\": [None, {\"dropout_iterations\": 60},\n",
    "         \t\t{\"rc_handling\" : \"maximum\", \"diff_type\":\"diff\"}],\n",
    "         \"out_annotation_all_outputs\": model_output_annotation,\n",
    "         }\n",
    "\n",
    "results = effect_from_model(**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again the returned value is a dictionary containing the results of the individual calculations, the keys are the names of the executed functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(results.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ISM\n",
    "\n",
    "`concise.effects.ism.ism`\n",
    "\n",
    "ISM offers two ways to calculate the difference between the outputs created by reference and alternative sequence and two different methods to select whether to use the output generated from the forward or from the reverse-complement sequences. You will have to choose those parameters according to your model design:\n",
    "\n",
    "\tdiff_type: \"log_odds\" or \"diff\". When set to 'log_odds' calculate scores based on log_odds, which assumes the model output is a probability. When set to 'diff' the model output for 'ref' is subtracted from 'alt'. Using 'log_odds' with outputs that are not in the range [0,1] nan will be returned.\n",
    "\trc_handling: \"average\" or \"maximum\". Either average over the predictions derived from forward and reverse-complement predictions ('average') or pick the prediction with the bigger absolute value ('maximum').\n",
    "\t\n",
    "This function returns a dictionary with the key `ism` which contains a pandas DataFrame containing the calculated values for each (selected) model output and input sequence. Using ISM in with diff_type 'log_odds' and rc_handling 'maximum' will produce predictions as used in [DeepSEA](http://www.nature.com/nmeth/journal/v12/n10/full/nmeth.3547.html). To calculate \"e-values\" as mentioned in DeepSEA the same ISM prediction has to be performed on a randomised set of 1 million 1000genomes, MAF-matched variants to get a background of predicted effects of random SNPs.\n",
    "\n",
    "## Gradient\n",
    "`concise.effects.gradient.gradient_pred`\n",
    "\n",
    "Based on the idea of [saliency maps](https://arxiv.org/pdf/1312.6034.pdf) the gradient-based prediction of variant effects uses the `gradient` function of the Keras backend to estimate the importance of a variant for a given output. This value is then multiplied by the input, as recommended by [Shrikumar et al., 2017](https://arxiv.org/pdf/1605.01713.pdf).\n",
    "\n",
    "This function returns a dictionary with three different entries:\n",
    "\n",
    "\tref: Gradient * input at the mutation position using the reference sequence. Forward or reverse-complement sequence is chose based on sequence direction caused the bigger absolute difference ('diff')\n",
    "\talt: Gradient * input at the mutation position using the alternative sequence. Forward or reverse-complement sequence is chose based on sequence direction caused the bigger absolute difference ('diff')\n",
    "\tdiff: 'alt' - 'ref'. Forward or reverse-complement sequence is chose based on sequence direction caused the bigger absolute difference.\n",
    "\t\n",
    "\n",
    "## Dropout\n",
    "`concise.effects.dropout.dropout_pred`\n",
    "\n",
    "This method is based on the ideas in [Gal et al.](https://arxiv.org/pdf/1506.02142.pdf) where dropout layers are also actived in the model prediction phase in order to estimate model uncertainty. The advantage of this method is that instead of a point estimate of the model output the distribution of the model output is estimated. This function has one additional parameter:\n",
    "\n",
    "\tdropout_iterations: Number of prediction iterations to be performed in order to estimate the output distribution. Values greater than 30 are recommended to get a reliable p-value.\n",
    "\t\n",
    "This function returns a dictionary with a set of measures of the model uncertainty in the variant position. The ones of interest are:\n",
    "\n",
    "\tdo_{ref, alt}_mean: Mean of the model predictions given the respective input sequence and dropout. Forward or reverse-complement sequences are chosen as for 'do_pv'.\n",
    "\tdo_{ref, alt}_var: Variance of the model predictions given the respective input sequence and dropout. Forward or reverse-complement sequences are chosen as for 'do_pv'.\n",
    "\tdo_diff: 'do_alt_mean' - 'do_alt_mean', which is an estimate similar to ISM using diff_type \"diff\".\n",
    "\tdo_pv: P-value of a paired t-test, comparing the predictions of ref with the ones of alt. Forward or reverse-complement sequences are chosen based on which pair has the lower p-value.\n",
    "\t\n",
    "The values in 'do_pv' should be used as score, where the lowest p-values indicate the strongest and most certain predicted effect. The p-values will be nan if the difference in means and variance for reference and alternative sequence are zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
