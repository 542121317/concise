{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Porting concise to keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Arguments:\n",
    "\n",
    "#  pooling_layer=\"sum\",\n",
    "#  nonlinearity=\"relu\",  # relu or exp\n",
    "#  optimizer=\"adam\",\n",
    "#  batch_size=32,\n",
    "#  n_epochs=3,\n",
    "#  early_stop_patience=None,\n",
    "#  n_iterations_checkpoint=20,\n",
    "#  # network details\n",
    "#  motif_length=9,\n",
    "#  n_motifs=6,\n",
    "#  step_size=0.01,\n",
    "#  step_epoch=10,\n",
    "#  step_decay=0.95,\n",
    "#  # - multi-task learning\n",
    "#  num_tasks=1,\n",
    "#  # - splines\n",
    "#  n_splines=None,\n",
    "#  share_splines=False,  # should the positional bias be shared across motifs\n",
    "#  spline_exp=False,    # use the exponential function\n",
    "#  # regularization\n",
    "#  lamb=1e-5,\n",
    "#  motif_lamb=1e-5,\n",
    "#  spline_lamb=1e-5,\n",
    "#  spline_param_lamb=1e-5,\n",
    "#  # initialization\n",
    "#  init_motifs=None,  # motifs to intialize\n",
    "#  init_motifs_scale=1,  # scale at which to initialize the weights\n",
    "#  # right scale\n",
    "#  nonlinearity_scale_factor=1,\n",
    "#  init_motif_bias=0,\n",
    "#  init_sd_motif=1e-2,\n",
    "#  init_sd_w=1e-3,         # initial weight scale of feature w or motif w\n",
    "#  # init_feat_w_lm=False,    # initalize features with a linear model\n",
    "#  # outuput detail\n",
    "#  print_every=100,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concise architecture\n",
    "\n",
    "Splines:\n",
    "- `spline_score = X_spline %*% spline_weights`\n",
    "- Transform:\n",
    "  - `exp(spline_score)`\n",
    "  - `spline_score + 1`\n",
    "\n",
    "Linear features:\n",
    "- `lm_feat = X_feat %*% feature_weights`\n",
    "\n",
    "Model:\n",
    "- conv2d, `padding = \"valid\", w = motif_base_weights`\n",
    "- activation: exp or relu, bias = motif_bias\n",
    "- elementwise_multiply: `hidden * spline_score`\n",
    "- pooling: max, sum or mean (accross the whole model)\n",
    "- Optionally: multiply by non-linear scaling factor (model fitting)\n",
    "- `pool_layer %*% motif_weights + X_feat %*% feature_weights + final_bias`\n",
    "- loss: mse\n",
    "- optimizer: Adam, optionally l-BFGS\n",
    "\n",
    "Regularization:\n",
    "- motif_base_weights, L1: motif_lamb\n",
    "- motif_weights, L1: lambd\n",
    "- spline_weights:\n",
    "  - `diag(t(spline_weights) %*% S %*% spline_weights)`, L2_mean: spline_lamb\n",
    "  - spline_weights, L2 / n_spline_tracks: spline_param_lamb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "ERROR (theano.sandbox.cuda): nvcc compiler not found on $PATH. Check your nvcc installation and try again.\n"
     ]
    }
   ],
   "source": [
    "## define my own layer\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "import numpy as np\n",
    "\n",
    "## TODO - how to intialize the motif weights + biases?\n",
    "## - init_motifs_scale\n",
    "## - init_motif_bias\n",
    "\n",
    "## TODO - setup init function\n",
    "\n",
    "## define the model here\n",
    "def DNA_conv_layer(seq_length, num_filters=(15, 15), conv_width=(15, 15), pool_width=35, L1=0, dropout=0.1):\n",
    "    \"\"\"\n",
    "    Very frequently used conv layer for sequence\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    assert len(num_filters) == len(conv_width)\n",
    "    for i, (nb_filter, nb_col) in enumerate(zip(num_filters, conv_width)):\n",
    "        conv_height = 4 if i == 0 else 1\n",
    "        # TODO - are these parameters correct?\n",
    "        model.add(Convolution2D(\n",
    "            nb_filter=nb_filter, nb_row=conv_height,\n",
    "            nb_col=nb_col, activation='linear',\n",
    "            init='he_normal', input_shape=(4, seq_length, 1),\n",
    "            dim_ordering='tf',\n",
    "            W_regularizer=l1(L1)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "    # for avg pooling - determine the maximum number of returned dimentions\n",
    "    # merge together\n",
    "    # model.add(AveragePooling2D(pool_size=(1, pool_width)))\n",
    "    # model.add(Flatten())\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def deep_wide_model(n_features, seq_length, loss=\"mse\", num_filters=(15, 15), conv_width=(15, 15),\n",
    "                    lr=0.001,\n",
    "                    pool_width=35, L1=0, L1_weights=0, dropout=0.1):\n",
    "    conv_model = DNA_conv_layer(seq_length, num_filters, conv_width, pool_width, L1, dropout)\n",
    "\n",
    "    # linear model\n",
    "    linear_model = Sequential()\n",
    "    linear_model.add(Activation(\"linear\", input_shape=(n_features, )))\n",
    "    merged = Merge([conv_model, linear_model], mode='concat')\n",
    "\n",
    "    final_model = Sequential()\n",
    "    final_model.add(merged)\n",
    "    final_model.add(Dense(output_dim=1, W_regularizer=l1(L1_weights)))\n",
    "\n",
    "    # model.add(Activation('linear'))\n",
    "    final_model.compile(optimizer=Adam(lr=lr), loss=loss)\n",
    "    return final_model\n",
    "\n",
    "def concise_model():\n",
    "    pass\n",
    "\n",
    "\n",
    "class Concise(Layer):\n",
    "    def __name__(self):\n",
    "        return \"Concise\"\n",
    "    \n",
    "    # TODO \n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super(MyLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.W = self.add_weight(shape=(input_shape[1], self.output_dim),\n",
    "                                 initializer='uniform',\n",
    "                                 trainable=True)\n",
    "        super(MyLayer, self).build(input_shape)  # Be sure to call this somewhere!\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        return K.dot(x, self.W)\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return (input_shape[0], self.output_dim)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {'output_dim': self.output_dim}\n",
    "        base_config = super(MyLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
