{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Porting concise to keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "- [ ] conv2d inizialization with pwm\n",
    "- [ ] smooth spline layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concise architecture\n",
    "\n",
    "Splines:\n",
    "- `spline_score = X_spline %*% spline_weights`\n",
    "- Transform:\n",
    "  - `exp(spline_score)`\n",
    "  - `spline_score + 1`\n",
    "\n",
    "Linear features:\n",
    "- `lm_feat = X_feat %*% feature_weights`\n",
    "\n",
    "Model:\n",
    "- conv2d, `padding = \"valid\", w = motif_base_weights`\n",
    "- activation: exp or relu, bias = motif_bias\n",
    "- elementwise_multiply: `hidden * spline_score`\n",
    "- pooling: max, sum or mean (accross the whole model)\n",
    "- Optionally: multiply by non-linear scaling factor (model fitting)\n",
    "- `pool_layer %*% motif_weights + X_feat %*% feature_weights + final_bias`\n",
    "- loss: mse\n",
    "- optimizer: Adam, optionally l-BFGS\n",
    "\n",
    "Regularization:\n",
    "- motif_base_weights, L1: motif_lamb\n",
    "- motif_weights, L1: lambd\n",
    "- spline_weights:\n",
    "  - `diag(t(spline_weights) %*% S %*% spline_weights)`, L2_mean: spline_lamb\n",
    "  - spline_weights, L2 / n_spline_tracks: spline_param_lamb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv2d pwm, motif initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras import layers as kl\n",
    "from keras import regularizers as kr\n",
    "from keras.initializers import Initializer\n",
    "## Arguments:\n",
    "## - init_motifs=[\"TATTTAT\", ..., \"ACTAAT\"]\n",
    "## - init_motifs_scale=1\n",
    "## - init_motif_bias=0\n",
    "## - init_sd_motif=1e-2\n",
    "\n",
    "## TODO - define a new layer or use the existing conv?\n",
    "## TODO - which conv functions do dragonn and deepcpg use?\n",
    "\n",
    "## write the initializer function\n",
    "\n",
    "## Num filters, width?\n",
    "\n",
    "## Regularizer\n",
    "l2_decay = 1\n",
    "l1_decay = 1\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "## TODO - implement\n",
    "def _check_pwm_list(pwm_list):\n",
    "    \"\"\"Check the input validity\n",
    "    \"\"\"\n",
    "\n",
    "    if invalid:\n",
    "        raise ValueError(\"pwm_list invalid\")\n",
    "    raise NotImplementedError()\n",
    "\n",
    "class PWMBiasInitializer(Initializer):\n",
    "    \n",
    "    def __init__(self, pwm_list=[]):\n",
    "        self.pwm_list = pwm_list\n",
    "        _check_pwm_list(pwm_list)\n",
    "        \n",
    "    def __call__(self, shape, dtype=None):\n",
    "        \n",
    "        ## TODO - think how to implement this\n",
    "        ## TODO get the number of bases...\n",
    "        max_effect = [pwm.max_effect() for pwm in self.pwm_list]\n",
    "        \n",
    "        ## TODO - get the number of biases\n",
    "        return K.constant(0, shape=shape, dtype=dtype)\n",
    "        \n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"pwm_list\": self.pwm_list\n",
    "        }\n",
    "\n",
    "## TODO program also a PWM-bases bias initializer\n",
    "## TODO - should we just initialize the mean???\n",
    "class PWMKernelInitializer(Initializer):\n",
    "    \"\"\"truncated normal distribution shifted by a PWM\n",
    "    \n",
    "    # Arguments\n",
    "        pwm_list: a list of PWM's or motifs\n",
    "        stddev: a python scalar or a scalar tensor. Standard deviation of the\n",
    "          random values to generate.\n",
    "        seed: A Python integer. Used to seed the random generator.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pwm_list=[], stddev=0.05, seed=None):\n",
    "        self.stddev = stddev\n",
    "        self.seed = seed\n",
    "        self.pwm_list = pwm_list\n",
    "        _check_pwm_list(pwm_list)\n",
    "        ## TODO - define a PWM class ?! - define it by using pwm matrix or motif sequences...\n",
    "        ## TODO - check the pwm_list & convert to array of the same shape\n",
    "        ##        - consider the shape and pad, trim accordingly\n",
    "        \n",
    "        \n",
    "    ## TODO - implement\n",
    "    def _to_pwm_array(self, pwm_list, shape, dtype=None):\n",
    "        \"\"\"Convert the pwm_list to a single pwm array\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "        return pwm_array\n",
    "\n",
    "    def __call__(self, shape, dtype=None):\n",
    "        return pwm_array + K.truncated_normal(shape, \n",
    "                                              mean=self._to_pwm_array(self.pwm_list, shape, dtype), \n",
    "                                              self.stddev,\n",
    "                                              dtype=dtype, seed=self.seed)\n",
    "\n",
    "    ## TODO - can PWM list be used? -> what are the reqirements of get_config?\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            'stddev': self.stddev,\n",
    "            'seed': self.seed, \n",
    "            'pwm_list': self.pwm_list\n",
    "        }\n",
    "\n",
    "conv_l = kl.Conv1D(filters=128, kernel_size=11, \n",
    "                   kernel_regularizer=kr.L1L2(l1=l1_decay, l2=l2_decay), ## Regularization\n",
    "                   padding=\"valid\",\n",
    "                   activation=\"relu\", \n",
    "                   kernel_initializer=PWMKernelInitializer(pwm_list, stddev=0.1), ## TODO\n",
    "                   bias_initializer=PWMBiasInitializer(pwm_list)\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smooth spline layer\n",
    "\n",
    "Arguments:\n",
    "-  n_splines=None,\n",
    "-  share_splines=False,  # should the positional bias be shared across motifs\n",
    "-  spline_exp=False,    # use the exponential function\n",
    "-  spline_lamb=1e-5,\n",
    "-  spline_param_lamb=1e-5,\n",
    "\n",
    "Computation:\n",
    "- `spline_score = X_spline %*% spline_weights`\n",
    "- Transform:\n",
    "  - `exp(spline_score)  ## spline_exp == True`\n",
    "  - `spline_score + 1   ## spline_exp == False`\n",
    "\n",
    "Regularization:\n",
    "- `diag(t(spline_weights) %*% S %*% spline_weights), L2_mean: spline_lamb`\n",
    "- `spline_weights, L2 / n_spline_tracks: spline_param_lamb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other todo's\n",
    "\n",
    "- [ ] implement vizualization techniques for GAM's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Arguments:\n",
    "\n",
    "#  pooling_layer=\"sum\",\n",
    "#  nonlinearity=\"relu\",  # relu or exp\n",
    "#  optimizer=\"adam\",\n",
    "#  batch_size=32,\n",
    "#  n_epochs=3,\n",
    "#  early_stop_patience=None,\n",
    "#  n_iterations_checkpoint=20,\n",
    "#  # network details\n",
    "#  motif_length=9,\n",
    "#  n_motifs=6,\n",
    "#  step_size=0.01,\n",
    "#  step_epoch=10,\n",
    "#  step_decay=0.95,\n",
    "#  # - multi-task learning\n",
    "#  num_tasks=1,\n",
    "#  # - splines\n",
    "#  n_splines=None,\n",
    "#  share_splines=False,  # should the positional bias be shared across motifs\n",
    "#  spline_exp=False,    # use the exponential function\n",
    "#  # regularization\n",
    "#  lamb=1e-5,\n",
    "#  motif_lamb=1e-5,\n",
    "#  spline_lamb=1e-5,\n",
    "#  spline_param_lamb=1e-5,\n",
    "#  # initialization\n",
    "#  init_motifs=None,  # motifs to intialize\n",
    "#  init_motifs_scale=1,  # scale at which to initialize the weights\n",
    "#  # right scale\n",
    "#  nonlinearity_scale_factor=1,\n",
    "#  init_motif_bias=0,\n",
    "#  init_sd_motif=1e-2,\n",
    "#  init_sd_w=1e-3,         # initial weight scale of feature w or motif w\n",
    "#  # init_feat_w_lm=False,    # initalize features with a linear model\n",
    "#  # outuput detail\n",
    "#  print_every=100,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "ERROR (theano.sandbox.cuda): nvcc compiler not found on $PATH. Check your nvcc installation and try again.\n"
     ]
    }
   ],
   "source": [
    "## define my own layer\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "import numpy as np\n",
    "\n",
    "## TODO - how to intialize the motif weights + biases?\n",
    "## - init_motifs_scale\n",
    "## - init_motif_bias\n",
    "\n",
    "## TODO - setup init function\n",
    "\n",
    "## define the model here\n",
    "def DNA_conv_layer(seq_length, num_filters=(15, 15), conv_width=(15, 15), pool_width=35, L1=0, dropout=0.1):\n",
    "    \"\"\"\n",
    "    Very frequently used conv layer for sequence\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    assert len(num_filters) == len(conv_width)\n",
    "    for i, (nb_filter, nb_col) in enumerate(zip(num_filters, conv_width)):\n",
    "        conv_height = 4 if i == 0 else 1\n",
    "        # TODO - are these parameters correct?\n",
    "        model.add(Convolution2D(\n",
    "            nb_filter=nb_filter, nb_row=conv_height,\n",
    "            nb_col=nb_col, activation='linear',\n",
    "            init='he_normal', input_shape=(4, seq_length, 1),\n",
    "            dim_ordering='tf',\n",
    "            W_regularizer=l1(L1)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "    # for avg pooling - determine the maximum number of returned dimentions\n",
    "    # merge together\n",
    "    # model.add(AveragePooling2D(pool_size=(1, pool_width)))\n",
    "    # model.add(Flatten())\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def deep_wide_model(n_features, seq_length, loss=\"mse\", num_filters=(15, 15), conv_width=(15, 15),\n",
    "                    lr=0.001,\n",
    "                    pool_width=35, L1=0, L1_weights=0, dropout=0.1):\n",
    "    conv_model = DNA_conv_layer(seq_length, num_filters, conv_width, pool_width, L1, dropout)\n",
    "\n",
    "    # linear model\n",
    "    linear_model = Sequential()\n",
    "    linear_model.add(Activation(\"linear\", input_shape=(n_features, )))\n",
    "    merged = Merge([conv_model, linear_model], mode='concat')\n",
    "\n",
    "    final_model = Sequential()\n",
    "    final_model.add(merged)\n",
    "    final_model.add(Dense(output_dim=1, W_regularizer=l1(L1_weights)))\n",
    "\n",
    "    # model.add(Activation('linear'))\n",
    "    final_model.compile(optimizer=Adam(lr=lr), loss=loss)\n",
    "    return final_model\n",
    "\n",
    "def concise_model():\n",
    "    pass\n",
    "\n",
    "\n",
    "class Concise(Layer):\n",
    "    def __name__(self):\n",
    "        return \"Concise\"\n",
    "    \n",
    "    # TODO \n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super(MyLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.W = self.add_weight(shape=(input_shape[1], self.output_dim),\n",
    "                                 initializer='uniform',\n",
    "                                 trainable=True)\n",
    "        super(MyLayer, self).build(input_shape)  # Be sure to call this somewhere!\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        return K.dot(x, self.W)\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return (input_shape[0], self.output_dim)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {'output_dim': self.output_dim}\n",
    "        base_config = super(MyLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
